{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 3 - Neural Machine Translation by Jointly Learning to Align and Translate\n",
        "\n",
        "In this third notebook on sequence-to-sequence models using PyTorch and TorchText, we'll be implementing the model from [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473). This model achives our best perplexity yet, ~27 compared to ~34 for the previous model.\n",
        "\n",
        "## Introduction\n",
        "\n",
        "As a reminder, here is the general encoder-decoder model:\n",
        "\n",
        "![](assets/seq2seq1.png)\n",
        "\n",
        "In the previous model, our architecture was set-up in a way to reduce \"information compression\" by explicitly passing the context vector, $z$, to the decoder at every time-step and by passing both the context vector and embedded input word, $d(y_t)$, along with the hidden state, $s_t$, to the linear layer, $f$, to make a prediction.\n",
        "\n",
        "![](assets/seq2seq7.png)\n",
        "\n",
        "Even though we have reduced some of this compression, our context vector still needs to contain all of the information about the source sentence. The model implemented in this notebook avoids this compression by allowing the decoder to look at the entire source sentence (via its hidden states) at each decoding step! How does it do this? It uses *attention*. \n",
        "\n",
        "Attention works by first, calculating an attention vector, $a$, that is the length of the source sentence. The attention vector has the property that each element is between 0 and 1, and the entire vector sums to 1. We then calculate a weighted sum of our source sentence hidden states, $H$, to get a weighted source vector, $w$. \n",
        "\n",
        "$$w = \\sum_{i}a_ih_i$$\n",
        "\n",
        "We calculate a new weighted source vector every time-step when decoding, using it as input to our decoder RNN as well as the linear layer to make a prediction. We'll explain how to do all of this during the tutorial.\n",
        "\n",
        "## Install required libraries\n",
        "\n",
        "We'll be coding up the models in PyTorch and using torchtext to help us do all of the pre-processing required. We'll also be using spaCy to assist in the tokenization of the data. Install spaCy tokenizers for English and German. "
      ],
      "metadata": {
        "id": "8WOguQh1PYB-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JZCGIz2jKycW",
        "outputId": "fe888077-419d-470a-d888-f95c10b6f3e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchdata\n",
            "  Downloading torchdata-0.4.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.4 MB 9.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchdata) (2.23.0)\n",
            "Collecting portalocker>=2.0.0\n",
            "  Downloading portalocker-2.5.1-py2.py3-none-any.whl (15 kB)\n",
            "Collecting urllib3>=1.25\n",
            "  Downloading urllib3-1.26.12-py2.py3-none-any.whl (140 kB)\n",
            "\u001b[K     |████████████████████████████████| 140 kB 6.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch==1.12.1 in /usr/local/lib/python3.7/dist-packages (from torchdata) (1.12.1+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.12.1->torchdata) (4.1.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchdata) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchdata) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchdata) (2.10)\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 49.4 MB/s \n",
            "\u001b[?25hInstalling collected packages: urllib3, portalocker, torchdata\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "Successfully installed portalocker-2.5.1 torchdata-0.4.1 urllib3-1.25.11\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (3.4.1)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.3)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (8.1.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.6)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.6.2)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.7)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.11.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.4.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.10.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (21.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.9.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.64.0)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.2)\n",
            "Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.1.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (57.4.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.21.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy) (3.8.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.25.11)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.7.8)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy) (2.0.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting en-core-web-sm==3.4.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.4.0/en_core_web_sm-3.4.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.8 MB 8.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from en-core-web-sm==3.4.0) (3.4.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.9.2)\n",
            "Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (4.1.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.0.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (21.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.11.3)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.0.8)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.3.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.21.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (57.4.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.0.7)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.4.2)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.0.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (4.64.0)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.4.4)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.6.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.0.6)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (8.1.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.10.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.0.10)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.8.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (5.2.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.25.11)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.0.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.7.8)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.0.1)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting de-core-news-sm==3.4.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-3.4.0/de_core_news_sm-3.4.0-py3-none-any.whl (14.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 14.6 MB 8.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from de-core-news-sm==3.4.0) (3.4.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (57.4.0)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (8.1.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (2.0.6)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (1.0.3)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (0.4.2)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (1.9.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (2.4.4)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (4.64.0)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (0.6.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (2.11.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (3.0.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (21.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (0.10.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (2.23.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (1.0.8)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (1.21.6)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (3.0.10)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (4.1.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (2.0.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (3.8.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (5.2.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (1.25.11)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (3.0.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (0.7.8)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (2.0.1)\n",
            "Installing collected packages: de-core-news-sm\n",
            "Successfully installed de-core-news-sm-3.4.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('de_core_news_sm')\n",
            "--2022-09-01 11:40:55--  http://www.quest.dcs.shef.ac.uk/wmt16_files_mmt/training.tar.gz\n",
            "Resolving www.quest.dcs.shef.ac.uk (www.quest.dcs.shef.ac.uk)... 143.167.8.76\n",
            "Connecting to www.quest.dcs.shef.ac.uk (www.quest.dcs.shef.ac.uk)|143.167.8.76|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://www.quest.dcs.shef.ac.uk/wmt16_files_mmt/training.tar.gz [following]\n",
            "--2022-09-01 11:40:56--  https://www.quest.dcs.shef.ac.uk/wmt16_files_mmt/training.tar.gz\n",
            "Connecting to www.quest.dcs.shef.ac.uk (www.quest.dcs.shef.ac.uk)|143.167.8.76|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1207136 (1.2M) [application/x-gzip]\n",
            "Saving to: ‘training.tar.gz’\n",
            "\n",
            "training.tar.gz     100%[===================>]   1.15M   638KB/s    in 1.8s    \n",
            "\n",
            "2022-09-01 11:40:58 (638 KB/s) - ‘training.tar.gz’ saved [1207136/1207136]\n",
            "\n",
            "URL transformed to HTTPS due to an HSTS policy\n",
            "--2022-09-01 11:40:58--  https://www.quest.dcs.shef.ac.uk/wmt16_files_mmt/validation.tar.gz\n",
            "Resolving www.quest.dcs.shef.ac.uk (www.quest.dcs.shef.ac.uk)... 143.167.8.76\n",
            "Connecting to www.quest.dcs.shef.ac.uk (www.quest.dcs.shef.ac.uk)|143.167.8.76|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 46329 (45K) [application/x-gzip]\n",
            "Saving to: ‘validation.tar.gz’\n",
            "\n",
            "validation.tar.gz   100%[===================>]  45.24K   170KB/s    in 0.3s    \n",
            "\n",
            "2022-09-01 11:40:59 (170 KB/s) - ‘validation.tar.gz’ saved [46329/46329]\n",
            "\n",
            "URL transformed to HTTPS due to an HSTS policy\n",
            "--2022-09-01 11:40:59--  https://www.quest.dcs.shef.ac.uk/wmt16_files_mmt/mmt16_task1_test.tar.gz\n",
            "Resolving www.quest.dcs.shef.ac.uk (www.quest.dcs.shef.ac.uk)... 143.167.8.76\n",
            "Connecting to www.quest.dcs.shef.ac.uk (www.quest.dcs.shef.ac.uk)|143.167.8.76|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 43905 (43K) [application/x-gzip]\n",
            "Saving to: ‘mmt16_task1_test.tar.gz’\n",
            "\n",
            "mmt16_task1_test.ta 100%[===================>]  42.88K   162KB/s    in 0.3s    \n",
            "\n",
            "2022-09-01 11:41:00 (162 KB/s) - ‘mmt16_task1_test.tar.gz’ saved [43905/43905]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!pip install -U torchdata\n",
        "!pip install -U spacy\n",
        "!python -m spacy download en_core_web_sm\n",
        "!python -m spacy download de_core_news_sm\n",
        "!wget \"http://www.quest.dcs.shef.ac.uk/wmt16_files_mmt/training.tar.gz\"\n",
        "!wget \"http://www.quest.dcs.shef.ac.uk/wmt16_files_mmt/validation.tar.gz\"\n",
        "!wget \"http://www.quest.dcs.shef.ac.uk/wmt16_files_mmt/mmt16_task1_test.tar.gz\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports\n",
        "\n",
        "Import evertyhing we need to train the model."
      ],
      "metadata": {
        "id": "GPHtY-SHPmEs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Iterable\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torchtext.datasets import multi30k, Multi30k\n",
        "\n",
        "import spacy\n",
        "import numpy as np\n",
        "\n",
        "import random\n",
        "import math\n",
        "import time"
      ],
      "metadata": {
        "id": "k1RFH02cPdW-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll set the random seeds for deterministic results."
      ],
      "metadata": {
        "id": "Uu2qGR59Poqm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SEED: int = 1234\n",
        "\n",
        "def random_seed(seed: int):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.determenistic = True\n",
        "\n",
        "random_seed(SEED)"
      ],
      "metadata": {
        "id": "WoTrdCGyPnAq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset we'll be using is the [Multi30k dataset](https://github.com/multi30k/dataset). This is a dataset with ~30,000 parallel English, German and French sentences, each with ~12 words per sentence. Currently, the URL for multi30k is broken so we have to replace with temporary links to download dataset. \n",
        "\n",
        "Next, we'll create the tokenizers. A tokenizer is used to turn a string containing a sentence into a list of individual tokens that make up that string, e.g. \"good morning!\" becomes [\"good\", \"morning\", \"!\"]. We'll start talking about the sentences being a sequence of tokens from now, instead of saying they're a sequence of words. What's the difference? Well, \"good\" and \"morning\" are both words and tokens, but \"!\" is a token, not a word. \n",
        "\n",
        "```\n",
        "tokenizer_de = get_tokenizer(\"spacy\", language=\"de_core_news_sm\")\n",
        "tokenizer_en = get_tokenizer(\"spacy\", language=\"en_core_web_sm\")\n",
        "```\n",
        "spaCy has model for each language (\"de_core_news_sm\" for German and \"en_core_web_sm\" for English) which need to be loaded so we can access the tokenizer of each model. "
      ],
      "metadata": {
        "id": "a_9GATe0Ps39"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SOURCE_LANGUAGE = 'de'\n",
        "TARGET_LANGUAGE = 'en'\n",
        "\n",
        "token_transform = {}\n",
        "vocab_transform = {}\n",
        "\n",
        "token_transform[SOURCE_LANGUAGE] = get_tokenizer(\"spacy\", language=\"de_core_news_sm\")\n",
        "token_transform[TARGET_LANGUAGE] = get_tokenizer(\"spacy\", language=\"en_core_web_sm\")\n",
        "\n",
        "\n",
        "def yield_tokens(data_iter: Iterable, language: str):\n",
        "    language_index = {SOURCE_LANGUAGE: 0, TARGET_LANGUAGE: 1}\n",
        "    for data_sample in data_iter:\n",
        "        yield token_transform[language](data_sample[language_index[language]])"
      ],
      "metadata": {
        "id": "0-gqh-bFPprZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we'll build the *vocabulary* for the source and target languages. The vocabulary is used to associate each unique token with an index (an integer). The vocabularies of the source and target languages are distinct.\n",
        "\n",
        "`torchtext`'s `build_vocab_from_iterator` would handle create of vocabulary for us. We have to set a yield fucntion for data processing where we will tokenize sentences. Add special tokens with their correspoding indexes with in the vocabulary.Using the `min_freq` argument, we only allow tokens that appear at least 2 times to appear in our vocabulary. Tokens that appear only once are converted into an `<unk>` (unknown) token. Special tokens: `<unk>` - unknown token which is not in the vocabulary, `<pad>` - padding token to make senteces equal lenght for dataloader, `<bos>` - begining of the sentence, `<eos>` - end of the sentence.   \n",
        "\n",
        "It is important to note that our vocabulary should only be built from the training set and not the validation/test set. This prevents \"information leakage\" into our model, giving us artifically inflated validation/test scores."
      ],
      "metadata": {
        "id": "EJYO8vK1PvLU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
        "special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']"
      ],
      "metadata": {
        "id": "a99WJm02Pt3w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for language in [SOURCE_LANGUAGE, TARGET_LANGUAGE]:\n",
        "    train_iter = Multi30k(root=\"/content\", split=\"train\", language_pair=(SOURCE_LANGUAGE, TARGET_LANGUAGE))\n",
        "    vocab_transform[language] = build_vocab_from_iterator(\n",
        "        yield_tokens(train_iter, language),\n",
        "        min_freq=2,\n",
        "        specials=special_symbols,\n",
        "        special_first=True\n",
        "    )\n",
        "\n",
        "for language in [SOURCE_LANGUAGE, TARGET_LANGUAGE]:\n",
        "    vocab_transform[language].set_default_index(UNK_IDX)"
      ],
      "metadata": {
        "id": "Rlr3SzRfPwh4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We also need to define a `torch.device`. This is used to tell torchText to put the tensors on the GPU or not. We use the `torch.cuda.is_available()` function, which will return `True` if a GPU is detected on our computer. We pass this `device` to the iterator.\n"
      ],
      "metadata": {
        "id": "8tuIDTR4PztE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "iuUdri-FPxS4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unlike previous version, `torchtext Field` did for us  tokenization and added special symbols, with current version of `torchtext>=0.12.0` you have to make a pipeline for text processing. `text_transform` is dictionary containg all necessary tranformation for source and target languages. Firsly, we tokenize sentences. Secondly, transform it into vocavulary indexes and add special symbol `<bos>` and `<eos>` for indication of start of senteces and ends. Finaly, transform it into `torch.Tensor`."
      ],
      "metadata": {
        "id": "wvScLfu2P4vM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sequential_transforms(*transforms):\n",
        "    def func(text_input: str):\n",
        "        for transform in transforms:\n",
        "            text_input = transform(text_input)\n",
        "        return text_input\n",
        "    return func\n",
        "\n",
        "def to_tensor(token_ids: List[int]):\n",
        "    return torch.cat((\n",
        "        torch.tensor([BOS_IDX]),\n",
        "        torch.tensor(token_ids),\n",
        "        torch.tensor([EOS_IDX]),\n",
        "    ))\n",
        "\n",
        "text_transform = {}\n",
        "\n",
        "for language in [SOURCE_LANGUAGE, TARGET_LANGUAGE]:\n",
        "    text_transform[language] = sequential_transforms(\n",
        "        token_transform[language],  # Tokenization\n",
        "        vocab_transform[language],  # Numericalization\n",
        "        to_tensor  # Add BOS/EOS and create tensor\n",
        "    )\n",
        "\n",
        "def collate_fn(batch):\n",
        "    source_batch, target_batch = [], []\n",
        "    for source_sample, target_sample in batch:\n",
        "        source_batch.append(text_transform[SOURCE_LANGUAGE](source_sample.rstrip(\"\\n\")))\n",
        "        target_batch.append(text_transform[TARGET_LANGUAGE](target_sample.rstrip(\"\\n\")))\n",
        "\n",
        "    source_batch = pad_sequence(source_batch, batch_first=False, padding_value=PAD_IDX)\n",
        "    target_batch = pad_sequence(target_batch, batch_first=False, padding_value=PAD_IDX)\n",
        "    return {\n",
        "        \"input\": source_batch,\n",
        "        \"target\": target_batch,\n",
        "    }"
      ],
      "metadata": {
        "id": "e_C_778gP2nf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Select a batch size for our dataloaders and make `collate_fn` function to pad our sequences. When we get a batch of examples using an iterator we need to make sure that all of the source sentences are padded to the same length, the same with the target sentences. Luckily, torchText iterators handle this for us! We use a `pad_sequence` to creates batches.\n"
      ],
      "metadata": {
        "id": "6tRjGx9DP806"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 64\n",
        "\n",
        "train_iter = Multi30k(root=\"/content\", split='train', language_pair=(SOURCE_LANGUAGE, TARGET_LANGUAGE))\n",
        "train_dataloader = DataLoader(train_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
        "\n",
        "valid_iter = Multi30k(root=\"/content\", split='valid', language_pair=(SOURCE_LANGUAGE, TARGET_LANGUAGE))\n",
        "valid_dataloader = DataLoader(valid_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
        "\n",
        "test_iter = Multi30k(root=\"/content\", split='test', language_pair=(SOURCE_LANGUAGE, TARGET_LANGUAGE))\n",
        "test_dataloader = DataLoader(test_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)"
      ],
      "metadata": {
        "id": "ovgI4ZMjP54_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building the Seq2Seq Model\n",
        "\n",
        "### Encoder\n",
        "\n",
        "First, we'll build the encoder. Similar to the previous model, we only use a single layer GRU, however we now use a *bidirectional RNN*. With a bidirectional RNN, we have two RNNs in each layer. A *forward RNN* going over the embedded sentence from left to right (shown below in green), and a *backward RNN* going over the embedded sentence from right to left (teal). All we need to do in code is set `bidirectional = True` and then pass the embedded sentence to the RNN as before. \n",
        "\n",
        "![](assets/seq2seq8.png)\n",
        "\n",
        "We now have:\n",
        "\n",
        "$$\\begin{align*}\n",
        "h_t^\\rightarrow &= \\text{EncoderGRU}^\\rightarrow(e(x_t^\\rightarrow),h_{t-1}^\\rightarrow)\\\\\n",
        "h_t^\\leftarrow &= \\text{EncoderGRU}^\\leftarrow(e(x_t^\\leftarrow),h_{t-1}^\\leftarrow)\n",
        "\\end{align*}$$\n",
        "\n",
        "Where $x_0^\\rightarrow = \\text{<sos>}, x_1^\\rightarrow = \\text{guten}$ and $x_0^\\leftarrow = \\text{<eos>}, x_1^\\leftarrow = \\text{morgen}$.\n",
        "\n",
        "As before, we only pass an input (`embedded`) to the RNN, which tells PyTorch to initialize both the forward and backward initial hidden states ($h_0^\\rightarrow$ and $h_0^\\leftarrow$, respectively) to a tensor of all zeros. We'll also get two context vectors, one from the forward RNN after it has seen the final word in the sentence, $z^\\rightarrow=h_T^\\rightarrow$, and one from the backward RNN after it has seen the first word in the sentence, $z^\\leftarrow=h_T^\\leftarrow$.\n",
        "\n",
        "The RNN returns `outputs` and `hidden`. \n",
        "\n",
        "`outputs` is of size **[src len, batch size, hid dim * num directions]** where the first `hid_dim` elements in the third axis are the hidden states from the top layer forward RNN, and the last `hid_dim` elements are hidden states from the top layer backward RNN. We can think of the third axis as being the forward and backward hidden states concatenated together other, i.e. $h_1 = [h_1^\\rightarrow; h_{T}^\\leftarrow]$, $h_2 = [h_2^\\rightarrow; h_{T-1}^\\leftarrow]$ and we can denote all encoder hidden states (forward and backwards concatenated together) as $H=\\{ h_1, h_2, ..., h_T\\}$.\n",
        "\n",
        "`hidden` is of size **[n layers * num directions, batch size, hid dim]**, where **[-2, :, :]** gives the top layer forward RNN hidden state after the final time-step (i.e. after it has seen the last word in the sentence) and **[-1, :, :]** gives the top layer backward RNN hidden state after the final time-step (i.e. after it has seen the first word in the sentence).\n",
        "\n",
        "As the decoder is not bidirectional, it only needs a single context vector, $z$, to use as its initial hidden state, $s_0$, and we currently have two, a forward and a backward one ($z^\\rightarrow=h_T^\\rightarrow$ and $z^\\leftarrow=h_T^\\leftarrow$, respectively). We solve this by concatenating the two context vectors together, passing them through a linear layer, $g$, and applying the $\\tanh$ activation function. \n",
        "\n",
        "$$z=\\tanh(g(h_T^\\rightarrow, h_T^\\leftarrow)) = \\tanh(g(z^\\rightarrow, z^\\leftarrow)) = s_0$$\n",
        "\n",
        "**Note**: this is actually a deviation from the paper. Instead, they feed only the first backward RNN hidden state through a linear layer to get the context vector/decoder initial hidden state. This doesn't seem to make sense to me, so we have changed it.\n",
        "\n",
        "As we want our model to look back over the whole of the source sentence we return `outputs`, the stacked forward and backward hidden states for every token in the source sentence. We also return `hidden`, which acts as our initial hidden state in the decoder."
      ],
      "metadata": {
        "id": "O_A9r4-YQK3y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_dim: int,\n",
        "        embed_dim: int,\n",
        "        enc_hidden_dim: int,\n",
        "        dec_hidden_dim: int,\n",
        "        dropout: float\n",
        "    ):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(input_dim, embed_dim)\n",
        "\n",
        "        self.rnn = nn.GRU(embed_dim, enc_hidden_dim, bidirectional=True)\n",
        "\n",
        "        self.fc = nn.Linear(enc_hidden_dim * 2, dec_hidden_dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        #x = [src len, batch size]\n",
        "        embeddings = self.embedding(x)\n",
        "        #embedded = [src len, batch size, emb dim]\n",
        "\n",
        "        outputs, hidden = self.rnn(embeddings)\n",
        "                \n",
        "        #outputs = [src len, batch size, hid dim * num directions]\n",
        "        #hidden = [n layers * num directions, batch size, hid dim]\n",
        "        \n",
        "        #hidden is stacked [forward_1, backward_1, forward_2, backward_2, ...]\n",
        "        #outputs are always from the last layer\n",
        "        \n",
        "        #hidden [-2, :, : ] is the last of the forwards RNN \n",
        "        #hidden [-1, :, : ] is the last of the backwards RNN\n",
        "        \n",
        "        #initial decoder hidden is final hidden state of the forwards and backwards \n",
        "        #  encoder RNNs fed through a linear layer\n",
        "        hidden = torch.cat((hidden[-2, :], hidden[-1, :]), dim=1)\n",
        "        hidden = self.fc(hidden)\n",
        "        hidden = torch.tanh(hidden)\n",
        "\n",
        "        return outputs, hidden"
      ],
      "metadata": {
        "id": "ZuEkj4uwQIU2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self, enc_hidden_dim: int, dec_hidden_dim: int):\n",
        "        super(Attention, self).__init__()\n",
        "\n",
        "        self.attn = nn.Linear(enc_hidden_dim * 2 + dec_hidden_dim, dec_hidden_dim)\n",
        "        self.v = nn.Linear(dec_hidden_dim, 1, bias=False)\n",
        "\n",
        "    def forward(self, hidden, encoder_outputs):\n",
        "        #hidden = [batch size, dec hid dim]\n",
        "        #encoder_outputs = [src len, batch size, enc hid dim * 2]\n",
        "\n",
        "        batch_size = encoder_outputs.size(1)\n",
        "        source_len = encoder_outputs.size(0)\n",
        "        \n",
        "        hidden = hidden.unsqueeze(1).repeat(1, source_len, 1)\n",
        "\n",
        "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
        "\n",
        "        #hidden = [batch size, src len, dec hid dim]\n",
        "        #encoder_outputs = [batch size, src len, enc hid dim * 2]\n",
        "        # print(hidden.shape, encoder_outputs.shape)\n",
        "        input = torch.cat((hidden, encoder_outputs), dim=2)\n",
        "        # torch.Size([64, 64, 512]) torch.Size([64, 27, 1024])\n",
        "        energy = torch.tanh(self.attn(input))\n",
        "        #energy = [batch size, src len, dec hid dim]\n",
        "\n",
        "        attention = self.v(energy).squeeze(2)\n",
        "        #attention= [batch size, src len]\n",
        "\n",
        "        return F.softmax(attention, dim=1)"
      ],
      "metadata": {
        "id": "LRTG235WiFiv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        output_dim: int,\n",
        "        embed_dim: int,\n",
        "        enc_hidden_dim: int,\n",
        "        dec_hidden_dim: int,\n",
        "        dropout: float,\n",
        "        attention\n",
        "    ):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.output_dim = output_dim\n",
        "        self.attention = attention\n",
        "\n",
        "        self.embedding = nn.Embedding(output_dim, embed_dim)\n",
        "        self.rnn = nn.GRU(enc_hidden_dim * 2 + embed_dim, dec_hidden_dim)\n",
        "\n",
        "        self.fc = nn.Linear(enc_hidden_dim * 2 + dec_hidden_dim + embed_dim, output_dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "    def forward(self, input, hidden, encoder_outputs):\n",
        "        #input = [batch size]\n",
        "        #hidden = [batch size, dec hid dim]\n",
        "        #encoder_outputs = [src len, batch size, enc hid dim * 2]\n",
        "\n",
        "        input = input.unsqueeze(0)\n",
        "        #input = [1, batch size]\n",
        "\n",
        "        embeddings = self.dropout(self.embedding(input))\n",
        "        #embedded = [1, batch size, emb dim]\n",
        "\n",
        "        attention = self.attention(hidden, encoder_outputs)\n",
        "        #attention = [batch size, src len]\n",
        "        \n",
        "        attention = attention.unsqueeze(1)\n",
        "        #attention = [batch size, 1, src len]\n",
        "\n",
        "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
        "        #encoder_outputs = [batch size, src len, enc hid dim * 2]\n",
        "\n",
        "        weighted = torch.bmm(attention, encoder_outputs)\n",
        "        #weighted = [batch size, 1, enc hid dim * 2]\n",
        "\n",
        "        weighted = weighted.permute(1, 0, 2)\n",
        "        #weighted = [1, batch size, enc hid dim * 2]\n",
        "\n",
        "        rnn_input = torch.cat((embeddings, weighted),dim=2)\n",
        "        #rnn_input = [1, batch size, (enc hid dim * 2) + emb dim]\n",
        "\n",
        "        output, hidden = self.rnn(rnn_input, hidden.unsqueeze(0))\n",
        "        \n",
        "        #output = [seq len, batch size, dec hid dim * n directions]\n",
        "        #hidden = [n layers * n directions, batch size, dec hid dim]\n",
        "        \n",
        "        #seq len, n layers and n directions will always be 1 in this decoder, therefore:\n",
        "        #output = [1, batch size, dec hid dim]\n",
        "        #hidden = [1, batch size, dec hid dim]\n",
        "        #this also means that output == hidden\n",
        "        assert (output == hidden).all()\n",
        "\n",
        "        embeddings = embeddings.squeeze(0)\n",
        "        output = output.squeeze(0)\n",
        "        weighted = weighted.squeeze(0)\n",
        "        #print(output.shape, weighted.shape, embeddings.shape)\n",
        "        predictions = self.fc(torch.cat((output, weighted, embeddings), dim=1))\n",
        "        #print(predictions.shape)\n",
        "\n",
        "        return predictions, hidden.squeeze(0)"
      ],
      "metadata": {
        "id": "wDTLkEB_t4zQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder: Encoder, decoder: Decoder, device: str):\n",
        "        super(Seq2Seq, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, data, teacher_forcing_ratio: float = 0.5):\n",
        "        for k, v in data.items():\n",
        "            data[k] = v.to(self.device)\n",
        "\n",
        "        batch_size = data[\"target\"].size(1)\n",
        "        target_len = data[\"target\"].size(0)\n",
        "        target_vocab_size = self.decoder.output_dim\n",
        "\n",
        "        outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(self.device)\n",
        "\n",
        "        encoder_outputs, hidden = self.encoder(data[\"input\"])\n",
        "\n",
        "        input = data[\"input\"][0, :]\n",
        "\n",
        "        for t in range(1, target_len):\n",
        "            #insert input token embedding, previous hidden state and all encoder hidden states\n",
        "            #receive output tensor (predictions) and new hidden state\n",
        "            output, hidden = self.decoder(input, hidden, encoder_outputs)\n",
        "\n",
        "            outputs[t] = output\n",
        "\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "\n",
        "            top1 = output.argmax(1)\n",
        "\n",
        "            input = data[\"target\"][t, :] if teacher_force else top1\n",
        "\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "6InceqoLxdJH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "INPUT_DIM = len(vocab_transform[SOURCE_LANGUAGE])\n",
        "OUTPUT_DIM = len(vocab_transform[TARGET_LANGUAGE])\n",
        "ENC_EMB_DIM = 256\n",
        "DEC_EMB_DIM = 256\n",
        "ENC_HIDDEN_DIM = 512\n",
        "DEC_HIDDEN_DIM = 512\n",
        "ENC_DROPOUT = 0.5\n",
        "DEC_DROPOUT = 0.5\n",
        "\n",
        "attnention = Attention(ENC_HIDDEN_DIM, DEC_HIDDEN_DIM)\n",
        "encoder = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HIDDEN_DIM, DEC_HIDDEN_DIM, ENC_DROPOUT)\n",
        "decoder = Decoder(OUTPUT_DIM, ENC_EMB_DIM, ENC_HIDDEN_DIM, DEC_HIDDEN_DIM, ENC_DROPOUT, attnention)\n",
        "\n",
        "model = Seq2Seq(encoder, decoder, DEVICE).to(DEVICE)"
      ],
      "metadata": {
        "id": "5ajyNWiZzjuu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def init_weights(m):\n",
        "    for name, param in m.named_parameters():\n",
        "        if 'weight' in name:\n",
        "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
        "        else:\n",
        "            nn.init.constant_(param.data, 0)\n",
        "            \n",
        "model.apply(init_weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5JkCWSqo0YOW",
        "outputId": "ec1c571c-4d6f-4583-b41f-56eebffa8df1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Seq2Seq(\n",
              "  (encoder): Encoder(\n",
              "    (embedding): Embedding(8014, 256)\n",
              "    (rnn): GRU(256, 512, bidirectional=True)\n",
              "    (fc): Linear(in_features=1024, out_features=512, bias=True)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (attention): Attention(\n",
              "      (attn): Linear(in_features=1536, out_features=512, bias=True)\n",
              "      (v): Linear(in_features=512, out_features=1, bias=False)\n",
              "    )\n",
              "    (embedding): Embedding(6191, 256)\n",
              "    (rnn): GRU(1280, 512)\n",
              "    (fc): Linear(in_features=1792, out_features=6191, bias=True)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZWHedsXw0h5n",
        "outputId": "cf74912f-540f-4fc6-cff8-75dbcb7cf96d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model has 21,170,223 trainable parameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.Adam(model.parameters())"
      ],
      "metadata": {
        "id": "3MPV1qL40jyv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)"
      ],
      "metadata": {
        "id": "Ga3xr8410lBu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, dataloader, optimizer, criterion, clip: float):\n",
        "    model.train()\n",
        "\n",
        "    epoch_loss = 0\n",
        "\n",
        "    for i, data in enumerate(dataloader):\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        output = model(data)\n",
        "\n",
        "        output_dim = output.size(-1)\n",
        "        # print(data[\"target\"].shape, output.shape)\n",
        "        # torch.Size([24, 64]) torch.Size([24, 64, 6191])\n",
        "        output = output[1:].view(-1, output_dim)\n",
        "        target = data[\"target\"][1:].view(-1)\n",
        "        \n",
        "        loss = criterion(output, target)\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(list(dataloader))"
      ],
      "metadata": {
        "id": "uCxGscvw0l3_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, dataloader, optimizer, criterion):\n",
        "    model.eval()\n",
        "\n",
        "    epoch_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for i, data in enumerate(dataloader):\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            output = model(data, 0)\n",
        "\n",
        "            output_dim = output.size(-1)\n",
        "\n",
        "            output = output[1:].view(-1, output_dim)\n",
        "            target = data[\"target\"][1:].view(-1)\n",
        "            \n",
        "            loss = criterion(output, target)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(list(dataloader))"
      ],
      "metadata": {
        "id": "QEllzMee0pf3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "metadata": {
        "id": "6e5cpIZg0rVm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "N_EPOCHS = 10\n",
        "CLIP = 1\n",
        "\n",
        "best_valid_loss = float(\"inf\")\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "    start_time = time.time()\n",
        "    train_loss = train(model, train_dataloader, optimizer, criterion, CLIP)\n",
        "    valid_loss = evaluate(model, valid_dataloader, optimizer, criterion)\n",
        "    \n",
        "    end_time = time.time()\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'model.pth')\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LAHJDZzs0sl_",
        "outputId": "b86f2c3d-df01-4398-cec7-596b1561fb3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/datapipes/iter/combining.py:249: UserWarning: Some child DataPipes are not exhausted when __iter__ is called. We are resetting the buffer and each child DataPipe will read from the start again.\n",
            "  \"the buffer and each child DataPipe will read from the start again.\", UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01 | Time: 1m 33s\n",
            "\tTrain Loss: 4.618 | Train PPL: 101.331\n",
            "\t Val. Loss: 4.281 |  Val. PPL:  72.348\n",
            "Epoch: 02 | Time: 1m 33s\n",
            "\tTrain Loss: 3.372 | Train PPL:  29.127\n",
            "\t Val. Loss: 3.624 |  Val. PPL:  37.506\n",
            "Epoch: 03 | Time: 1m 33s\n",
            "\tTrain Loss: 2.663 | Train PPL:  14.334\n",
            "\t Val. Loss: 3.370 |  Val. PPL:  29.071\n",
            "Epoch: 04 | Time: 1m 33s\n",
            "\tTrain Loss: 2.200 | Train PPL:   9.023\n",
            "\t Val. Loss: 3.292 |  Val. PPL:  26.909\n",
            "Epoch: 05 | Time: 1m 33s\n",
            "\tTrain Loss: 1.852 | Train PPL:   6.372\n",
            "\t Val. Loss: 3.387 |  Val. PPL:  29.590\n",
            "Epoch: 06 | Time: 1m 34s\n",
            "\tTrain Loss: 1.579 | Train PPL:   4.850\n",
            "\t Val. Loss: 3.415 |  Val. PPL:  30.404\n",
            "Epoch: 07 | Time: 1m 33s\n",
            "\tTrain Loss: 1.381 | Train PPL:   3.979\n",
            "\t Val. Loss: 3.542 |  Val. PPL:  34.533\n",
            "Epoch: 08 | Time: 1m 33s\n",
            "\tTrain Loss: 1.225 | Train PPL:   3.404\n",
            "\t Val. Loss: 3.652 |  Val. PPL:  38.564\n",
            "Epoch: 09 | Time: 1m 33s\n",
            "\tTrain Loss: 1.093 | Train PPL:   2.982\n",
            "\t Val. Loss: 3.765 |  Val. PPL:  43.182\n",
            "Epoch: 10 | Time: 1m 33s\n",
            "\tTrain Loss: 0.949 | Train PPL:   2.584\n",
            "\t Val. Loss: 3.873 |  Val. PPL:  48.078\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load('model.pth'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Peur7TZF0uOP",
        "outputId": "f943be0f-d758-45b1-cb04-93ed9d7e3e99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_iter = Multi30k(root=\"/content\", split='test', language_pair=(SOURCE_LANGUAGE, TARGET_LANGUAGE))\n",
        "test_dataloader = DataLoader(test_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
        "\n",
        "test_loss = evaluate(model, test_dataloader, optimizer, criterion)\n",
        "\n",
        "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mu_UVVBt6weL",
        "outputId": "920d3580-fa9f-4549-b430-c1dfd535c701"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| Test Loss: 3.277 | Test PPL:  26.498 |\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "\n",
        "epoch_loss = 0\n",
        "with torch.no_grad():\n",
        "    for i, batch in enumerate(test_dataloader):\n",
        "        for k, v in batch.items():\n",
        "            batch[k] = v.to(DEVICE)\n",
        "\n",
        "        output = model(batch, 0)\n",
        "        break"
      ],
      "metadata": {
        "id": "lldeB1NA6yDK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idx2word = vocab_transform[TARGET_LANGUAGE].get_itos()"
      ],
      "metadata": {
        "id": "ljknvRxN61LS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def reverse(index):\n",
        "    string = []\n",
        "    for idx in index:\n",
        "        if idx > 3:\n",
        "            string.append(idx2word[idx])\n",
        "    return \" \".join(string)"
      ],
      "metadata": {
        "id": "qCGakYeV6305"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ground_truths = []\n",
        "for i in range(64):\n",
        "    gt = batch[\"target\"][:, i]\n",
        "    ground_truths.append(reverse(gt.detach().cpu().numpy()))"
      ],
      "metadata": {
        "id": "4Grl_TXD641Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = []\n",
        "for i in range(64):\n",
        "    test_output = output[:, i].argmax(-1)\n",
        "    predictions.append(reverse(test_output.detach().cpu().numpy()))\n"
      ],
      "metadata": {
        "id": "2k4Y5idq651Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for ground_truth, prediction in zip(ground_truths, predictions):\n",
        "    print(ground_truth)\n",
        "    print(prediction)\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Frsd8uZT663B",
        "outputId": "cf9f858c-eba0-4c31-92ca-11af4ca5a381"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A man in an orange hat starring at something .\n",
            "A man with an orange hat , something something something .\n",
            "\n",
            "A Boston Terrier is running on lush green grass in front of a white fence .\n",
            "A golden dog runs over grass grass in front of a white fence . .\n",
            "\n",
            "A girl in karate uniform breaking a stick with a front kick .\n",
            "A girl in a karate uniform is a a with a gun . .\n",
            "\n",
            "Five people wearing winter jackets and helmets stand in the snow , with in the background .\n",
            "Five people in winter winter and helmets stand in the snow with the background . background . background . background . background . background .\n",
            "\n",
            "People are fixing the roof of a house .\n",
            "People are on a building . . .\n",
            "\n",
            "A man in light colored clothing photographs a group of men wearing dark suits and hats standing around a woman dressed in a gown .\n",
            "A bright red - skinned man is a a of men in a woman in a woman in a dress dress around around around a woman in a light .\n",
            "\n",
            "A group of people standing in front of an igloo .\n",
            "A group of people standing in front of a campfire . .\n",
            "\n",
            "A boy in a red uniform is attempting to avoid getting out at home plate , while the catcher in the blue uniform is attempting to catch him .\n",
            "A boy in a red uniform tries to catch a baseball while the the baseball while the the baseball tries to catch him .\n",
            "\n",
            "A guy works on a building .\n",
            "A guy working on a building . building . building .\n",
            "\n",
            "A man in a vest is sitting in a chair and holding magazines .\n",
            "A man in a vest is sitting on a chair holding a . .\n",
            "\n",
            "A mother and her young song enjoying a beautiful day outside .\n",
            "A mother and her son enjoy a beautiful day .\n",
            "\n",
            "Men playing volleyball , with one player missing the ball but hands still in the air .\n",
            "Men are playing volleyball as a man got his hands in the air . . . . . .\n",
            "\n",
            "A woman holding a bowl of food in a kitchen .\n",
            "A woman in a kitchen holding a bowl of food . food .\n",
            "\n",
            "Man sitting using tool at a table in his home .\n",
            "A man sitting on a table with a rifle in his lap . .\n",
            "\n",
            "Three people sit in a cave .\n",
            "Three people are sitting in a a .\n",
            "\n",
            "A girl in a jean dress is walking along a raised balance beam .\n",
            "A girl in a suit is over a wooden laundry wooden wooden wooden .\n",
            "\n",
            "A blond holding hands with a guy in the sand .\n",
            "A woman holding a man in the sand . .\n",
            "\n",
            "A woman in a gray sweater and black baseball cap is standing in line at a shop .\n",
            "A woman in a and sweater and a black cap stands in a store in a store . .\n",
            "\n",
            "The person in the striped shirt is mountain climbing .\n",
            "The person in the striped shirt is climbing on a mountain .\n",
            "\n",
            "Two men pretend to be while women look on .\n",
            "Two men are as they are while women watch . . . .\n",
            "\n",
            "People standing outside of a building .\n",
            "People standing outside outside building building . building . building . building . building . building . building . building . building\n",
            "\n",
            "A teenager plays her trumpet on the field at a game .\n",
            "A young boy plays in a game game in a game .\n",
            "\n",
            "A woman does a somersault on a trampoline on the beach .\n",
            "A woman does a flip on a trampoline on the beach . beach . beach . beach . beach . beach . beach .\n",
            "\n",
            "A man is standing by a group of video games in a bar .\n",
            "A man is standing at a a in a a .\n",
            "\n",
            "A woman uses a drill while another man takes her picture .\n",
            "A woman using a a sink while a man man is taking photographs .\n",
            "\n",
            "A woman in a pink sweater and an apron , cleaning a table with a sponge .\n",
            "A woman in a pink sweater and a apron is a a table with a coffee . .\n",
            "\n",
            "A man cutting branches of trees .\n",
            "A man is trimming some trees . trees .\n",
            "\n",
            "Group of Asian boys wait for meat to cook over barbecue .\n",
            "A group of young boys are waiting to get food grill preparing to be food .\n",
            "\n",
            "Women , wearing traditional clothing , are native life .\n",
            "Women dressed in traditional traditional play playing the . . . . . . . .\n",
            "\n",
            "One man holds another man 's head down and prepares to punch him in the face .\n",
            "A man is flinging another man 's to take his face and his his his face . .\n",
            "\n",
            "Six people ride mountain bikes through a jungle environment .\n",
            "Six people are riding through a a .\n",
            "\n",
            "2 blond girls are sitting on a ledge in a crowded plaza .\n",
            "2 girls are sitting on a boulder in a busy square . . . . . . .\n",
            "\n",
            "A child is splashing in the water\n",
            "A child splashes in the water . water . water .\n",
            "\n",
            "Three people sit at a picnic table outside of a building painted like a union jack .\n",
            "Three people are sitting at a picnic table in front of a building that is been been been been . .\n",
            "\n",
            "3 boys are standing on a pier in their bathing suits .\n",
            "3 boys stand in the pier on a pier . . . . . . .\n",
            "\n",
            "An employee is handing a woman a bag while she is browsing through fish on ice at a street market .\n",
            "A woman hands a a a a while while while the on the on the . .\n",
            "\n",
            "A pretty woman plays a .\n",
            "A pregnant woman is playing a a game .\n",
            "\n",
            "a building , a uniformed security guard looks at the camera from behind a fence .\n",
            "A man in a a in a a in a car behind a fence . a fence . . . .\n",
            "\n",
            "The young lady is looking at the pizza .\n",
            "The young lady is looking at a kitchen .\n",
            "\n",
            "A shirtless man in shorts is fishing while standing on some rocks .\n",
            "A shirtless man and shorts stands on a marketplace and fishing . . . . . . .\n",
            "\n",
            "A girl wearing a life vest floats in water .\n",
            "A girl in a life jacket is in the water . water .\n",
            "\n",
            "A man in uniform and a man in a blue shirt are standing in front of a truck .\n",
            "A man in a and and a man in a blue shirt standing in front of a cart .\n",
            "\n",
            "People sit inside a train .\n",
            "People are sitting on a train .\n",
            "\n",
            "A kid swings with his feet up in the air in a forest .\n",
            "A young boy is in the air in the air in a forest .\n",
            "\n",
            "A man in a red shirt entering an establishment .\n",
            "A man in a red shirt is a a .\n",
            "\n",
            "Two men wearing swim trunks jump in the air at a populated beach .\n",
            "Two men in bathing jumping jumping on a a a a beach . . . . . .\n",
            "\n",
            "A toddler is cooking with another person .\n",
            "A small child is food with another person . .\n",
            "\n",
            "A father - figure and two children outside their home doing yard work such as using a hoe on the grass and planting a tree .\n",
            "A female and and two children perform their band before their band placed in the grass with a with a tree . tree . tree .\n",
            "\n",
            "A man cooking food on the stove .\n",
            "A man preparing food food at the stove . stove . stove . stove . stove . stove . stove . stove .\n",
            "\n",
            "A man in jeans at the beach playing with a red ball .\n",
            "A man in jeans plays a beach with a red ball . beach .\n",
            "\n",
            "People walking down sidewalk next to a line of stores .\n",
            "People walking on a walkway next to next to a next to . . . . .\n",
            "\n",
            "A performs a flip while being towed at high speed .\n",
            "A professional does a flip as he rides a rope . . .\n",
            "\n",
            "A large group of people fill a street .\n",
            "A large crowd of people are down a street .\n",
            "\n",
            "A man on a tag line going into the water .\n",
            "A man on a - suit is walking in the water . .\n",
            "\n",
            "A woman in jeans walks by a bus with an ad depicting a woman peering over her sunglasses .\n",
            "A woman in jeans is past a a woman who is a woman from the the over her hands over her hands over her hands . .\n",
            "\n",
            "A man in a pink shirt is sitting in the grass and a ball is in the air .\n",
            "A man in a pink shirt is on the grass and a ball in the the . .\n",
            "\n",
            "A car parked at the beach .\n",
            "A sunny car is car on the beach . . . . . . . .\n",
            "\n",
            "Two men wearing black in a city\n",
            "Two men in black in a city .\n",
            "\n",
            "The man in the yellow pants is raising his arms .\n",
            "The man in the pants is his his hands .\n",
            "\n",
            "Two men wearing hats and using walking sticks are walking near a body of water during .\n",
            "Two men wearing hats and snowshoes are digging next to a body of water . . . . . .\n",
            "\n",
            "A cheerleading team doing a routine on chairs .\n",
            "A professional is performing a on a .\n",
            "\n",
            "A boy is playing checkers with an adult shown off - screen while a girl looks on .\n",
            "A boy plays with an adult at his lap while a young girl watches .\n",
            "\n",
            "A crowd of people out for some fun in public park .\n",
            "A group of people are gathered around in a public setting .\n",
            "\n",
            "A man sits on a bench holding his dog and looking at the water .\n",
            "A man sits on a bench while his dog holding the water . . . . . .\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lSAj-59h68CQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}